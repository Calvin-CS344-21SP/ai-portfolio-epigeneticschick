{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of 007-softmax.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Calvin-CS344-21SP/ai-portfolio-epigeneticschick/blob/main/Copy_of_007_softmax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFzvVN7XeAux"
      },
      "source": [
        "# `007-softmax`\n",
        "\n",
        "Task: practice using the `softmax` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ2FUSC5eAu9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkiiOgUheAvC"
      },
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0KZw9mHeAvF"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hpoOWXleAvI"
      },
      "source": [
        "Let's see how `softmax` behaves!\n",
        "\n",
        "Try this example: `x = tensor([1., 2., 3.])`\n",
        "\n",
        "1. Compare the result of `softmax(x)` with the result of `x.exp() / x.exp().sum()`. Are they close? They are the exact same.\n",
        "2. What happens to the output of `softmax` when you give it `x + 1` instead of `x`? What happens if you add 100 instead? (Do this without changing `x`) It does not change. \n",
        "3. *optional*: What happens to the output of `x.exp() / x.exp().sum()` when you add 1 to x? When you add 100? \n",
        "4. What happens when you multiply `x` by a constant like 0.5 or 3.0 before passing it to `softmax`? Compare this situation with the situation in question 2. The answer is much different. The situation is different because rather than maintaining their distant numerically. The distance could possibly grow due to the fact that it has been mutiplied \n",
        "\n",
        "**Note: you'll need to specify the axis: `torch.softmax(x, dim=0)`.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezQ6YCXmeAvN"
      },
      "source": [
        "## Solution\n",
        "\n",
        "Add code and Markdown cells for each of the listed tasks above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o78TBlSnjxmD"
      },
      "source": [
        "x = tensor([1.,2.,3.])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbYJoaMTkRhs",
        "outputId": "44f7533a-6c99-4d64-cce5-77c4ef60b945"
      },
      "source": [
        "print(torch.softmax(x, dim=0))\n",
        "print(x.exp()/x.exp().sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n",
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lquf8AsjkToO",
        "outputId": "e6a3d024-eef8-4263-866c-b50de919954e"
      },
      "source": [
        "print(torch.softmax(x+1,dim=0))\n",
        "print(torch.softmax(x+100,dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n",
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTvWe7KJlZ4k",
        "outputId": "5bf118e3-f22d-49ae-d16d-d64d2115c802"
      },
      "source": [
        "new_x = x *3.5\n",
        "print(torch.softmax(new_x, dim=0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([8.8437e-04, 2.9286e-02, 9.6983e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uuS70DOeAvQ"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwhxYSQ9eAvS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac0fc25-3f66-4a9c-ccf2-1fdd1106b204"
      },
      "source": [
        "x2 = tensor([1., 0.,])\n",
        "x3 = x2 - 1\n",
        "x3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0., -1.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2cqGsf-eAvW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50f3cd62-56eb-40ef-b62c-b6bd512c46e2"
      },
      "source": [
        "x4 = x2 * 2\n",
        "x4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2., 0.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jhHTgTp_mQXE",
        "outputId": "908ca5cb-5cc1-4c20-d90e-2343154f27c0"
      },
      "source": [
        "print(torch.softmax(x2, dim=0))\n",
        "print(torch.softmax(x3, dim=0))\n",
        "print(torch.softmax(x4, dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.7311, 0.2689])\n",
            "tensor([0.7311, 0.2689])\n",
            "tensor([0.8808, 0.1192])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp5P4B1KeAvY"
      },
      "source": [
        "1. Are `softmax(x2)` and `softmax(x3)` the same or different? How could you tell without having to try it? They are the same. I could tell because one is being subtracted from x2 to make x3 meaning that they maintain their distance. \n",
        "2. Are `softmax(x2)` and `softmax(x4)` the same or different? How could you tell without having to try it? They are different, becuase x4 is x2 mutiplied by a constant. Mutiplication by a constant alters the distance between numbers in the tensors. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvpUBp3ReAvZ"
      },
      "source": [
        "## Extension *optional*\n",
        "\n",
        "1. Try to prove your observation in \\#2 by symbolically simplifying the expression `softmax(logits + c)` and seeing if you can get `softmax(logits)`. Remember that `softmax(x) = exp(x) / exp(x).sum()` and `exp(a + b) = exp(a)exp(b)`.\n",
        "\n",
        "2. Why does `exp(x + 100) / exp(x + 100).sum()` not work, while it does work for `torch.softmax`? Can you think of what `torch.softmax` might be doing to make sure that works?"
      ]
    }
  ]
}