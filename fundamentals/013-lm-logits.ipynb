{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kcarnold/cs344/blob/main/portfolio/fundamentals/013-lm-logits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jc8Qlh1TEgC"
   },
   "source": [
    "# `013` Language Models and Logits\n",
    "\n",
    "Task: Ask a language model for the most likely next tokens.\n",
    "\n",
    "This notebook follows up on `012-tokenization`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8_8RWp3TX-8"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUvTIxyWTdBF"
   },
   "source": [
    "We'll be using the HuggingFace Transformers library, which provides a (mostly) consistent interface to many different language models. We'll focus on the OpenAI GPT-2 model, famous for OpenAI's assertion that it was \"too dangerous\" to release in full.\n",
    "\n",
    "[Documentation](https://huggingface.co/transformers/model_doc/gpt2.html) for the model and tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWy--2nwhWPy",
    "outputId": "4d55a3c6-fb9d-4d6f-ee1e-a9e918c98e38"
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "osKgPaDwhaN4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiNKbIh8hyDg"
   },
   "source": [
    "### Download and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "IM5o_4w1hfyV"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\", add_prefix_space=True) # smaller version of GPT-2\n",
    "# Alternative to add_prefix_space is to use `is_split_into_words=True`\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-Z9_U0LUEVQ",
    "outputId": "cb7d4eb7-bc54-4583-dd10-e0cb185494da"
   },
   "outputs": [],
   "source": [
    "print(f\"The model has {model.num_parameters():,d} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OOUiz_PsUZgS"
   },
   "source": [
    "## Task\n",
    "\n",
    "Consider the following phrase: \"This weekend I plan to\".\n",
    "\n",
    "1. Convert the phrase into token ids.\n",
    "2. Use the `forward` method of the `model`. Explain the shape of `model_output.logits`.\n",
    "3. Pull out the logits corresponding to the *last* token in the input phrase. Identify the id of the most likely next token.\n",
    "4. Find what token the model thinks is the most likely.\n",
    "5. Use the `topk` method to find the top-20 most likely choices for the next token. \n",
    "6. Write a function that is given a phrase and a *k* and returns the top *k* most likely next tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JS7Z-DjoUiLK"
   },
   "outputs": [],
   "source": [
    "#phrase = \"In a shocking finding, scientists discovered a herd of unicorns living in\"\n",
    "phrase = \"This weekend I plan to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "QpyeKakrjfpt"
   },
   "outputs": [],
   "source": [
    "input_ids = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEy1QBTDotjU",
    "outputId": "70a3312b-1bc7-47b0-c1d6-4739c93f56c0"
   },
   "outputs": [],
   "source": [
    "model_output = model.forward(tensor([input_ids]))\n",
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2EHMGJFPS05B"
   },
   "outputs": [],
   "source": [
    "# since we only have a single sequence (batch size of 1), let's collapse the batch dimension.\n",
    "logits = model_output.logits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJWXqQLLkCyP",
    "outputId": "b969f2ed-2289-48c2-e717-b4802a493c5d"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "xhL9qP4HpJlx",
    "outputId": "8a014913-7577-4b14-e987-c0c126994df5"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsI_Tz0ipglx"
   },
   "source": [
    "## Analysis\n",
    "\n",
    "What would be required to generate more than one token? What decisions would you have to make?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNL1iHs_phOB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOVes22Uvu4sPkNN1/P8/pg",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "013-lm-logits",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
