{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Copy of 008-softmax-2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Calvin-CS344-21SP/ai-portfolio-epigeneticschick/blob/main/Copy_of_008_softmax_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWE3-kaTnitP"
      },
      "source": [
        "# `008-softmax-2`\n",
        "\n",
        "Task: more practice using the `softmax` function, and connect it with the `sigmoid` function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbvIXf-KnitY"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ro3NE-hTnita"
      },
      "source": [
        "import torch\n",
        "from torch import tensor\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONWfluDonitc"
      },
      "source": [
        "## Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hyvDyOSnitd"
      },
      "source": [
        "Try this example: `x = tensor([0.1, 0.2, 0.3])`\n",
        "\n",
        "1. If `p=softmax(x)`, what is `p.sum()`? Can you get `p.sum()` to change by changing `x`? Can you make `p.min()` be less than 0? The higher you make the values of x the smaller p.min is. P.sum stays the same the entire time no matter changes. \n",
        "2. Try making an `xx` that's the same as `x` except that `xx[2] = 100`, and let `p = softmax(xx)` again. How does `p[2]` compare with `p[0]` and `p[1]`? The tensors for p[0] and p[1] are 0 but the tensor for p[2] is 1.\n",
        "3. Try `torch.sigmoid(tensor(0.1))`. Can you write an expression that uses `torch.softmax` to get the same output? Yes, I was able to.\n",
        "\n",
        "**Hint for \\#3**: Give `sigmoid` a two-element `tensor`. One of those elements will be 0."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic8dgkqJnitf"
      },
      "source": [
        "## Solution\n",
        "\n",
        "Add code and Markdown cells for each of the listed tasks above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgF07JGdpZXA"
      },
      "source": [
        "x= tensor([0.1,100, 100])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POv6SABrpgPW",
        "outputId": "3b9282ca-a548-4ba7-96d7-485258fe328c"
      },
      "source": [
        "p=torch.softmax(x,dim=0)\n",
        "print(p.sum())\n",
        "print(p.min())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "tensor(1.9618e-44)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef7KYJxwqYmT",
        "outputId": "d3497528-eedb-46e9-ce29-695cc169a6c4"
      },
      "source": [
        "xx = tensor([0.1,0.2,200])\n",
        "p=torch.softmax(xx,dim=0)\n",
        "print(p[2])\n",
        "print(p[1])\n",
        "print(p[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.)\n",
            "tensor(0.)\n",
            "tensor(0.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDilna7vsHoE",
        "outputId": "40603ea8-576e-4187-bde0-16bdf0002d80"
      },
      "source": [
        "torch.softmax(tensor([0.1,0]),dim=0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5250, 0.4750])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS9JHcHNr840",
        "outputId": "5af52d86-0d8c-4bb0-853b-26432431f9f2"
      },
      "source": [
        "torch.sigmoid(tensor(0.1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5250)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SemjWD7dt-Zc",
        "outputId": "cf49c9a2-c6cb-4855-a87c-cf2782023b85"
      },
      "source": [
        "print(torch.log_softmax(x,dim=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-100.5931,   -0.6931,   -0.6931])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQ50dK4snitf"
      },
      "source": [
        "## Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftE8sZ8Ynitg"
      },
      "source": [
        "1. A valid probability distribution has no negative numbers and sums to 1. Is `softmax(x)` a valid probability distribution? Yes, it is. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcYS-7LWnith"
      },
      "source": [
        "2. Sometimes `x` is called the \"logits\" and `x.softmax().log()` (or `x.log_softmax()`) is called the \"logprobs\", short for \"log probabilities\". Compute the logits, logprobs, and probabilities for `x` in the example above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orNmmVEQniti"
      },
      "source": [
        "3. In light of your observations about `xx[2]` and `p[2]` above, why might `softmax` be an appropriate name for this function? It might be a good name because based upon the outputs you can tell which is the max number out of the tensor. "
      ]
    }
  ]
}